{
    "model_name": "llama3.1-8b-instruct",
    "layers": [14],
    "clamp_norm_factor": 1,
    "layer_selection": "all",
    "fact_token": "last",
    "lr": 2e-4,
    "v_num_grad_steps": 80,
    "v_lr": 0.08,
    "v_loss_layer": 31,
    "v_weight_decay": 5e-4,
    "optim_num_step": 50,
    "ex_data_num": 20,
    "rewrite_module_tmp": "model.layers.{}.mlp.down_proj",
    "layer_module_tmp": "model.layers.{}",
    "mlp_module_tmp": "model.layers.{}.mlp",
    "attn_module_tmp": "model.layers.{}.self_attn",
    "ln_f_module": "model.norm",
    "lm_head_module": "lm_head",
    "window_size": 25,
    "overlap": 5,
    "select_first_window": 1,
    "top_k_windows": 2
}